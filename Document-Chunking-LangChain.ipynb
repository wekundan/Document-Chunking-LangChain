{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "J0INoJq6Ak_l",
      "metadata": {
        "id": "J0INoJq6Ak_l"
      },
      "source": [
        "# Data Science Assignment: Exploring Text Splitters in LangChain\n",
        "\n",
        "**Introduction**\n",
        "\n",
        "This assignment focuses on text-splitting strategies using the LangChain framework. Text splitters are essential for dividing large texts into smaller, manageable chunks for processing with language models. In this assignment, you will use several built-in LangChain splitters to split sample texts and analyze the results.\n",
        "\n",
        "You will work with the following splitters:\n",
        "- Split text based on a fixed character count.\n",
        "- Split text recursively, preserving semantic boundaries like sentences.\n",
        "- Split text based on semantic meaning (requires a model for embeddings).\n",
        "- Split Markdown text based on headers.\n",
        "- Split HTML text based on headers."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-dbvtTQwAk_r",
      "metadata": {
        "id": "-dbvtTQwAk_r"
      },
      "source": [
        "**Sample Texts**\n",
        "\n",
        "Below are sample texts for each splitter:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "Uv82V-xGAk_s",
      "metadata": {
        "id": "Uv82V-xGAk_s"
      },
      "outputs": [],
      "source": [
        "plain_text = \"\"\"\n",
        "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\n",
        "\n",
        "Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DL04uBEYAk_s",
      "metadata": {
        "id": "DL04uBEYAk_s"
      },
      "source": [
        "- **Markdown Text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "JRHYBca_Ak_t",
      "metadata": {
        "id": "JRHYBca_Ak_t"
      },
      "outputs": [],
      "source": [
        "markdown_text = \"\"\"\n",
        "# Introduction\n",
        "This is the introduction section.\n",
        "\n",
        "## Subsection 1\n",
        "This is the first subsection under the introduction.\n",
        "\n",
        "### Details\n",
        "Here are some details about subsection 1.\n",
        "\n",
        "## Subsection 2\n",
        "This is the second subsection.\n",
        "\n",
        "# Conclusion\n",
        "This is the conclusion section.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nU1A3ekbAk_t",
      "metadata": {
        "id": "nU1A3ekbAk_t"
      },
      "source": [
        "- **HTML Text**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "kGNs4L7EAk_t",
      "metadata": {
        "id": "kGNs4L7EAk_t"
      },
      "outputs": [],
      "source": [
        "html_text = \"\"\"\n",
        "<html>\n",
        "<body>\n",
        "<h1>Introduction</h1>\n",
        "<p>This is the introduction section.</p>\n",
        "<h2>Subsection 1</h2>\n",
        "<p>This is the first subsection under the introduction.</p>\n",
        "<h3>Details</h3>\n",
        "<p>Here are some details about subsection 1.</p>\n",
        "<h2>Subsection 2</h2>\n",
        "<p>This is the second subsection.</p>\n",
        "<h1>Conclusion</h1>\n",
        "<p>This is the conclusion section.</p>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_a45GO_3Ak_u",
      "metadata": {
        "id": "_a45GO_3Ak_u"
      },
      "source": [
        "**Task 1: Character-Based Splitting**\n",
        "\n",
        "Use a Character based splitter to split the `plain_text` into chunks of approximately 100 characters, with a chunk overlap of 20 characters. Print the first three chunks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Pt7nIVHBAk_v",
      "metadata": {
        "id": "Pt7nIVHBAk_v"
      },
      "source": [
        "**Task 2: Recursive Character Splitting**\n",
        "\n",
        "Use Recursive Character splitter to split the `plain_text` into chunks of approximately 100 characters, with a chunk overlap of 20 characters. This splitter attempts to preserve semantic boundaries (e.g., sentences). Print the first three chunks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QOEe8U4oAk_w",
      "metadata": {
        "id": "QOEe8U4oAk_w"
      },
      "source": [
        "**Task 3: Semantic Chunking**\n",
        "\n",
        "Use the Semantic Chunker to split the `plain_text` based on semantic meaning. You will need to use an embedding model for this splitter. Print the first three chunks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QoRZDcVjAk_w",
      "metadata": {
        "id": "QoRZDcVjAk_w"
      },
      "source": [
        "**Task 4: Markdown Header Splitting**\n",
        "\n",
        "Use a Markdown Splitter to split the `markdown_text` based on Markdown headers. Print the resulting chunks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aKR5YObLAk_x",
      "metadata": {
        "id": "aKR5YObLAk_x"
      },
      "source": [
        "**Task 5: HTML Text Splitting**\n",
        "\n",
        "Use a HTML Text Splitter to split the `html_text` based on HTML headers. Print the resulting chunks.\n",
        "\n",
        "Hint: Specify the headers to split on, such as `h1`, `h2`, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "9f763627",
      "metadata": {
        "id": "9f763627"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Character Based Chunks (first 3):\n",
            "\n",
            "Chunk 0:\n",
            "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore\n",
            "\n",
            "Chunk 1:\n",
            "incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation\n",
            "\n",
            "Chunk 2:\n",
            "nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\n",
            "\n",
            "Duis aute irure dolor\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Task 1: Character Based splitting \n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "char_splitter = CharacterTextSplitter(separator=\" \", chunk_size = 100, chunk_overlap= 20)\n",
        "\n",
        "char_chunks = char_splitter.split_text(plain_text)\n",
        "\n",
        "print(\"Character Based Chunks (first 3):\\n\")\n",
        "for i, chunk in enumerate(char_chunks[:3]):\n",
        "    print(f'Chunk {i}:\\n{chunk}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e833840",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recursive Chunks (first 3):\n",
            "\n",
            "Chunk 0:\n",
            "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore\n",
            "\n",
            "Chunk 1:\n",
            "ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco\n",
            "\n",
            "Chunk 2:\n",
            "ullamco laboris nisi ut aliquip ex ea commodo consequat.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Task 2: Recursive Character Splitting\n",
        "\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "rec_splitter = RecursiveCharacterTextSplitter(chunk_size= 100, chunk_overlap= 20)\n",
        "recur_chunks = rec_splitter.split_text(plain_text)\n",
        "\n",
        "print(\"Recursive Chunks (first 3):\\n\")\n",
        "for i, chunk in enumerate(recur_chunks[:3]):\n",
        "    print(f'Chunk {i}:\\n{chunk}\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "896489f8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Semantic Chunks (first 3):\n",
            "\n",
            "Chunk 1:\n",
            "\n",
            "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\n",
            "\n",
            "Chunk 2:\n",
            "Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur.\n",
            "\n",
            "Chunk 3:\n",
            "Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Task 3: Semantic Chunking\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "sentences = sent_tokenize(plain_text)\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "embeddings = model.encode(sentences)\n",
        "\n",
        "num_chunks = 3\n",
        "kmeans = KMeans(n_clusters=num_chunks, random_state=0)\n",
        "labels = kmeans.fit_predict(embeddings)\n",
        "\n",
        "clusters = {i: [] for i in range(num_chunks)}\n",
        "for sentence, label in zip(sentences, labels):\n",
        "    clusters[label].append(sentence)\n",
        "\n",
        "print(\"Semantic Chunks (first 3):\\n\")\n",
        "for i in range(num_chunks):\n",
        "    chunk = ' '.join(clusters[i])\n",
        "    print(f\"Chunk {i+1}:\\n{chunk}\\n\")\n",
        "\n",
        "\n",
        " \n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "3415c4f4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Markdown Header Chunks:\n",
            "\n",
            "Chunk 1:\n",
            "This is the introduction section.\n",
            "\n",
            "Chunk 2:\n",
            "This is the first subsection under the introduction.\n",
            "\n",
            "Chunk 3:\n",
            "Here are some details about subsection 1.\n",
            "\n",
            "Chunk 4:\n",
            "This is the second subsection.\n",
            "\n",
            "Chunk 5:\n",
            "This is the conclusion section.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Task 4: Markdown Header Splitting\n",
        "\n",
        "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
        "\n",
        "headers_to_split_on = [\n",
        "    (\"#\", \"Header1\"),\n",
        "    (\"##\", \"Header2\"),\n",
        "    (\"###\", \"Header3\"),\n",
        "]\n",
        "\n",
        "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
        "\n",
        "md_chunks = markdown_splitter.split_text(markdown_text)\n",
        "\n",
        "print(\"Markdown Header Chunks:\\n\")\n",
        "for i, chunk in enumerate(md_chunks):\n",
        "    print(f\"Chunk {i+1}:\\n{chunk.page_content}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "6abce271",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HTML Header Chunks:\n",
            "\n",
            "Chunk 1:\n",
            "Introduction\n",
            "\n",
            "Chunk 2:\n",
            "This is the introduction section.\n",
            "\n",
            "Chunk 3:\n",
            "Subsection 1\n",
            "\n",
            "Chunk 4:\n",
            "This is the first subsection under the introduction.\n",
            "\n",
            "Chunk 5:\n",
            "Details\n",
            "\n",
            "Chunk 6:\n",
            "Here are some details about subsection 1.\n",
            "\n",
            "Chunk 7:\n",
            "Subsection 2\n",
            "\n",
            "Chunk 8:\n",
            "This is the second subsection.\n",
            "\n",
            "Chunk 9:\n",
            "Conclusion\n",
            "\n",
            "Chunk 10:\n",
            "This is the conclusion section.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Task 5: HTML Header Splitting\n",
        "\n",
        "from langchain.text_splitter import HTMLHeaderTextSplitter\n",
        "\n",
        "headers_to_split_on = [(\"h1\", \"Header 1\"), (\"h2\", \"Header 2\"), (\"h3\", \"Header 3\")]\n",
        "\n",
        "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
        "html_chunks = html_splitter.split_text(html_text)\n",
        "\n",
        "print(\"HTML Header Chunks:\\n\")\n",
        "for i, chunk in enumerate(html_chunks):\n",
        "    print(f\"Chunk {i+1}:\\n{chunk.page_content}\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
